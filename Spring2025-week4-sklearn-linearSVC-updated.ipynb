{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - build LinearSVC model with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to use the Sci-kit Learn (sklearn) package to build linearSVC model, rank features, and use the model for prediction. We will be using the Kaggle sentiment data again.\n",
    "\n",
    "Note that sklearn actually provides two SVM algorithms: SVC and LinearSVC. \n",
    "\n",
    "The SVC module allows for choosing nonlinear kernels, and it uses one-vs-one strategy for multi-class classification.\n",
    "\n",
    "The LinearSVC module uses the linear kernel, and it uses one-vs-all strategy for multi-class classification, so linearSVC is generally faster than SVC. Since linear kernel works better for text classification in general, this tutorial demonstrates how to use LinearSVC for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step is the same as the NB script\n",
    "\n",
    "# read in the training data\n",
    "# the data set includes four columns: PhraseId, SentenceId, Phrase, Sentiment\n",
    "# In this data set a sentence is further split into phrases \n",
    "# in order to build a sentiment classification model\n",
    "# that can not only predict sentiment of sentences but also shorter phrases\n",
    "\n",
    "# A data example:\n",
    "# PhraseId SentenceId Phrase Sentiment\n",
    "# 1 1 A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .1\n",
    "\n",
    "# the Phrase column includes the training examples\n",
    "# the Sentiment column includes the training labels\n",
    "# \"0\" for very negative\n",
    "# \"1\" for negative\n",
    "# \"2\" for neutral\n",
    "# \"3\" for positive\n",
    "# \"4\" for very positive\n",
    "\n",
    "\n",
    "import pandas as p\n",
    "import numpy as np\n",
    "train=p.read_csv(\"data/kaggle-sentiment/train.tsv\", delimiter='\\t')\n",
    "y=train['Sentiment'].values\n",
    "X=train['Phrase'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Split train/test data for hold-out test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93636,) (93636,) (62424,) (62424,)\n",
      "almost in a class with that of Wilde\n",
      "3\n",
      "escape movie\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# this step is the same as the NB script\n",
    "\n",
    "# check the sklearn documentation for train_test_split\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "# \"test_size\" : float, int, None, optional\n",
    "# If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. \n",
    "# If int, represents the absolute number of test samples. \n",
    "# If None, the value is set to the complement of the train size. \n",
    "# By default, the value is set to 0.25. The default will change in version 0.21. It will remain 0.25 only if train_size is unspecified, otherwise it will complement the specified train_size.    \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(X_train[0])\n",
    "print(y_train[0])\n",
    "print(X_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample output from the code above:\n",
    "\n",
    "(93636,) (93636,) (62424,) (62424,)\n",
    "almost in a class with that of Wilde\n",
    "3\n",
    "escape movie\n",
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.1 Data Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     1     2     3     4]\n",
      " [ 4141 16449 47718 19859  5469]]\n"
     ]
    }
   ],
   "source": [
    "# this step is the same as the NB script\n",
    "\n",
    "# Check how many training examples in each category\n",
    "# this is important to see whether the data set is balanced or skewed\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(np.asarray((unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample output shows that the data set is skewed with 47718/93636=51% \"neutral\" examples. All other categories are smaller.\n",
    "\n",
    "{0, 1, 2, 3, 4}\n",
    "[[    0  4141]\n",
    " [    1 16449]\n",
    " [    2 47718]\n",
    " [    3 19859]\n",
    " [    4  5469]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step is the same as the NB script\n",
    "\n",
    "# sklearn contains two vectorizers\n",
    "\n",
    "# CountVectorizer can give you Boolean or TF vectors\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "# TfidfVectorizer can give you TF or TFIDF vectors\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "# Read the sklearn documentation to understand all vectorization options\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# several commonly used vectorizer setting\n",
    "\n",
    "#  unigram boolean vectorizer, set minimum document frequency to 5\n",
    "unigram_bool_vectorizer = CountVectorizer(encoding='latin-1', binary=True, min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram term frequency vectorizer, set minimum document frequency to 5\n",
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram and bigram term frequency vectorizer, set minimum document frequency to 5\n",
    "gram12_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1,2), min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram tfidf vectorizer, set minimum document frequency to 5\n",
    "unigram_tfidf_vectorizer = TfidfVectorizer(encoding='latin-1', use_idf=True, min_df=5, stop_words='english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: Vectorize the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93636, 11967)\n",
      "11967\n",
      "[('class', 1858), ('wilde', 11742), ('derring', 2802), ('chilling', 1764), ('affecting', 313), ('meanspirited', 6557), ('personal', 7662), ('low', 6296), ('involved', 5602), ('worth', 11868)]\n",
      "5224\n"
     ]
    }
   ],
   "source": [
    "# this step is the same as the NB script\n",
    "\n",
    "# The vectorizer can do \"fit\" and \"transform\"\n",
    "# fit is a process to collect unique tokens into the vocabulary\n",
    "# transform is a process to convert each document to vector based on the vocabulary\n",
    "# These two processes can be done together using fit_transform(), or used individually: fit() or transform()\n",
    "\n",
    "# fit vocabulary in training documents and transform the training documents into vectors\n",
    "X_train_vec = unigram_count_vectorizer.fit_transform(X_train)\n",
    "print(X_train_vec.shape)\n",
    "\n",
    "\n",
    "# check the size of the constructed vocabulary\n",
    "print(len(unigram_count_vectorizer.vocabulary_))\n",
    "\n",
    "# print out the first 10 items in the vocabulary\n",
    "print(list(unigram_count_vectorizer.vocabulary_.items())[:10])\n",
    "\n",
    "# check word index in vocabulary\n",
    "print(unigram_count_vectorizer.vocabulary_.get('imaginative'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample output:\n",
    "\n",
    "(93636, 11967)\n",
    "[[0 0 0 ..., 0 0 0]]\n",
    "11967\n",
    "[('imaginative', 5224), ('tom', 10809), ('smiling', 9708), ('easy', 3310), ('diversity', 3060), ('impossibly', 5279), ('buy', 1458), ('sentiments', 9305), ('households', 5095), ('deteriorates', 2843)]\n",
    "5224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: Vectorize the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62424, 11967)\n"
     ]
    }
   ],
   "source": [
    "# this step is the same as the NB script\n",
    "\n",
    "# use the vocabulary constructed from the training data to vectorize the test data. \n",
    "# Therefore, use \"transform\" only, not \"fit_transform\", \n",
    "# otherwise \"fit\" would generate a new vocabulary from the test data\n",
    "\n",
    "X_test_vec = unigram_count_vectorizer.transform(X_test)\n",
    "\n",
    "# print out #examples and #features in the test set\n",
    "print(X_test_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample output:\n",
    "\n",
    "(62424, 14324)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Train a LinearSVC classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(C=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LinearSVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.LinearSVC.html\">?<span>Documentation for LinearSVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LinearSVC(C=1)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC(C=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the LinearSVC module\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# initialize the LinearSVC model\n",
    "svm_clf = LinearSVC(C=1)\n",
    "\n",
    "# use the training data to train the model\n",
    "svm_clf.fit(X_train_vec,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4.1 Interpret a trained LinearSVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very positive words\n",
      "(2.0142068736849783, 'perfection')\n",
      "(1.9796890798654854, 'miraculous')\n",
      "(1.8779306360018486, 'glorious')\n",
      "(1.677504410914676, 'masterfully')\n",
      "(1.650455163018035, 'masterful')\n",
      "(1.6472517990059208, 'phenomenal')\n",
      "(1.6143481059315383, 'flawless')\n",
      "(1.6104866405407123, 'refreshes')\n",
      "(1.599861966032188, 'astonish')\n",
      "(1.5631089253679928, 'stunning')\n",
      "(1.5234301668483532, 'heralds')\n",
      "(1.5156776631204303, 'masterpiece')\n",
      "(1.5068116113627559, 'proud')\n",
      "(1.4950645642045213, 'magnificent')\n",
      "(1.483797843365202, 'encourage')\n",
      "(1.4751453552680465, 'succeeded')\n",
      "(1.460400082928849, 'extraordinary')\n",
      "(1.4207476993073285, 'captivated')\n",
      "(1.4163136283342785, 'magnetic')\n",
      "(1.411922534365247, 'zings')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## interpreting LinearSVC models\n",
    "## http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
    "\n",
    "## LinearSVC uses a one-vs-all strategy to extend the binary SVM classifier to multi-class problems\n",
    "## for the Kaggle sentiment classification problem, there are five categories 0,1,2,3,4 with 0 as very negative and 4 very positive\n",
    "## LinearSVC builds five binary classifier, \"very negative vs. others\", \"negative vs. others\", \"neutral vs. others\", \"positive vs. others\", \"very positive vs. others\", \n",
    "## and then pick the most confident prediction as the final prediction.\n",
    "\n",
    "## Linear SVC also ranks all features based on their contribution to distinguish the two concepts in each binary classifier\n",
    "## For category \"0\" (very negative), get all features and their weights and sort them in increasing order\n",
    "feature_ranks = sorted(zip(svm_clf.coef_[4], unigram_count_vectorizer.get_feature_names_out()), reverse=True)\n",
    "\n",
    "## get the top \"very negative\" words\n",
    "topN = 20\n",
    "topN_words = feature_ranks[:topN]\n",
    "print(\"Very positive words\")\n",
    "for i in range(0, len(topN_words)):\n",
    "    print(topN_words[i])\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with weights greater than 0.3: 1551\n"
     ]
    }
   ],
   "source": [
    "# Count the number of words with weights greater than 0.3 in svm_clf.coef_[4]\n",
    "count = sum(1 for weight in svm_clf.coef_[4] if weight > 0.3)\n",
    "print(\"Number of words with weights greater than 0.3:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with weights between 0.5 and 0.51:\n",
      "enlightening\n",
      "calamity\n",
      "technical\n",
      "clarity\n",
      "spirited\n",
      "ultimate\n",
      "aggravating\n",
      "worthy\n",
      "concentrates\n",
      "apocalypse\n",
      "successfully\n",
      "documentary\n",
      "goal\n",
      "cuts\n",
      "wide\n",
      "buoyant\n",
      "sprightly\n",
      "package\n",
      "cleverest\n",
      "infectious\n",
      "loopy\n",
      "happily\n",
      "invaluable\n",
      "richly\n",
      "elemental\n",
      "polanski\n",
      "freshness\n",
      "apex\n"
     ]
    }
   ],
   "source": [
    "# Filter words with weights between 0.3 and 0.31\n",
    "filtered_words = [word for weight, word in feature_ranks if 0.5 <= weight <= 0.51]\n",
    "\n",
    "print(\"Words with weights between 0.5 and 0.51:\")\n",
    "for word in filtered_words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code similar to the above sample code \n",
    "# and print out the top 20 \"very positive\" words\n",
    "\n",
    "# Your code starts here\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Test the LinearSVC classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6236703831859541"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the classifier on the test data set, print accuracy score\n",
    "\n",
    "svm_clf.score(X_test_vec,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5104447007561195"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using sklearn to calculate the majority vote baseline\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "dummy_clf.score(X_test, y_test)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  918  1221   697    82    13]\n",
      " [  701  4080  5504   514    25]\n",
      " [  195  2106 27081  2310   172]\n",
      " [   34   396  6048  5532  1058]\n",
      " [    3    51   590  1772  1321]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.31      0.38      2931\n",
      "           1       0.52      0.38      0.44     10824\n",
      "           2       0.68      0.85      0.75     31864\n",
      "           3       0.54      0.42      0.48     13068\n",
      "           4       0.51      0.35      0.42      3737\n",
      "\n",
      "    accuracy                           0.62     62424\n",
      "   macro avg       0.55      0.46      0.49     62424\n",
      "weighted avg       0.60      0.62      0.60     62424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix and classification report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = svm_clf.predict(X_test_vec)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[0,1,2,3,4])\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['0','1','2','3','4']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5.1 Interpret the prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.01697015 -0.5075494   0.22330632 -0.97509521 -1.24753842]\n",
      "2\n",
      "A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .\n"
     ]
    }
   ],
   "source": [
    "## get the confidence scores for all test examples from each of the five binary classifiers\n",
    "svm_confidence_scores = svm_clf.decision_function(X_test_vec)\n",
    "## get the confidence score for the first test example\n",
    "print(svm_confidence_scores[0])\n",
    "\n",
    "## sample output: array([-1.05306321, -0.62746206,  0.31074854, -0.89709483, -1.08343089]\n",
    "## because the confidence score is the highest for category 2, \n",
    "## the prediction should be 2. \n",
    "\n",
    "## Confirm by printing out the actual prediction\n",
    "print(y_test[0])\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89601509, 0.09938126, 0.00233236, 0.00127423, 0.00099706])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output prediction probs\n",
    "# https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html#sphx-glr-auto-examples-calibration-plot-calibration-curve-py\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "svm_calibrated = CalibratedClassifierCV(svm_clf) \n",
    "svm_calibrated.fit(X_train_vec, y_train)\n",
    "y_test_proba = svm_calibrated.predict_proba(X_test_vec)\n",
    "y_test_proba[51597]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 0:\n",
      "Phrase: 51597 that her life is meaningless , vapid and devoid of substance , in a movie that is definitely meaningless , vapid and devoid of substance, Confidence Score: 0.896015091018129\n",
      "Phrase: 62006 her life is meaningless , vapid and devoid of substance , in a movie that is definitely meaningless , vapid and devoid of substance, Confidence Score: 0.896015091018129\n",
      "Phrase: 11427 is meaningless , vapid and devoid of substance , in a movie that is definitely meaningless , vapid and devoid of substance, Confidence Score: 0.8760157946708909\n",
      "Phrase: 45463 Astonishing is n't the word -- neither is incompetent , incoherent or just plain crap, Confidence Score: 0.8732493417773652\n",
      "Phrase: 5239 is the sort of low-grade dreck that usually goes straight to video -- with a lousy script , inept direction , pathetic acting , poorly dubbed dialogue and murky cinematography , complete with visible boom mikes, Confidence Score: 0.8726813488021061\n",
      "Phrase: 28246 is absolutely and completely ridiculous and an insult to every family whose mother has suffered through the horrible pains of a death by cancer, Confidence Score: 0.8390239471931592\n",
      "Phrase: 15061 he had a bad run in the market or a costly divorce , because there is no earthly reason other than money why this distinguished actor would stoop so low, Confidence Score: 0.8357679181108265\n",
      "Phrase: 42826 in a movie that is definitely meaningless , vapid and devoid of substance, Confidence Score: 0.8285380351287793\n",
      "Phrase: 26877 Godard 's ode to tackling life 's wonderment is a rambling and incoherent manifesto about the vagueness of topical excess ... In Praise of Love remains a ponderous and pretentious endeavor that 's unfocused and tediously exasperating, Confidence Score: 0.8211396245898263\n",
      "Phrase: 47872 Godard 's ode to tackling life 's wonderment is a rambling and incoherent manifesto about the vagueness of topical excess ... In Praise of Love remains a ponderous and pretentious endeavor that 's unfocused and tediously exasperating ., Confidence Score: 0.8211396245898263\n",
      "\n",
      "Category 1:\n",
      "Phrase: 62105 Each story is built on a potentially interesting idea , but the first two are ruined by amateurish writing and acting , while the third feels limited by its short running time, Confidence Score: 0.9168653825754243\n",
      "Phrase: 9034 The film 's hero is a bore and his innocence soon becomes a questionable kind of inexcusable dumb innocence ., Confidence Score: 0.914965107870629\n",
      "Phrase: 24494 So verbally flatfooted and so emotionally predictable or bland that it plays like the standard made-for-TV movie ., Confidence Score: 0.9112487265272519\n",
      "Phrase: 55417 The kind of film that leaves you scratching your head in amazement over the fact that so many talented people could participate in such an ill-advised and poorly executed idea ., Confidence Score: 0.9067596414416823\n",
      "Phrase: 61563 is an unsettling sight , and indicative of his , if you will , out-of-kilter character , who rambles aimlessly through ill-conceived action pieces ., Confidence Score: 0.9061085256559556\n",
      "Phrase: 4949 feels like a teenybopper Ed Wood film , replete with the pubescent scandalous innuendo and the high-strung but flaccid drama ., Confidence Score: 0.9027481531015805\n",
      "Phrase: 4332 feels like a teenybopper Ed Wood film , replete with the pubescent scandalous innuendo and the high-strung but flaccid drama, Confidence Score: 0.9027481531015805\n",
      "Phrase: 38317 Makes the same mistake as the music industry it criticizes , becoming so slick and watered-down it almost loses what made you love it in the first place, Confidence Score: 0.8999605399785358\n",
      "Phrase: 61663 Makes the same mistake as the music industry it criticizes , becoming so slick and watered-down it almost loses what made you love it, Confidence Score: 0.8972134893041066\n",
      "Phrase: 36779 like a teenybopper Ed Wood film , replete with the pubescent scandalous innuendo and the high-strung but flaccid drama, Confidence Score: 0.8907522242421034\n",
      "\n",
      "Category 2:\n",
      "Phrase: 33845 light and sugary that were it a Macy 's Thanksgiving Day Parade balloon, Confidence Score: 0.9286068221226869\n",
      "Phrase: 11617 is so light and sugary that were it a Macy 's Thanksgiving Day Parade balloon, Confidence Score: 0.9286068221226869\n",
      "Phrase: 58591 think the central story of Brendan Behan is that he was a bisexual sweetheart before he took to drink, Confidence Score: 0.925708750961733\n",
      "Phrase: 40402 Does anyone much think the central story of Brendan Behan is that he was a bisexual sweetheart before he took to drink ?, Confidence Score: 0.9227858266271578\n",
      "Phrase: 25030 Does anyone much think the central story of Brendan Behan is that he was a bisexual sweetheart before he took to drink, Confidence Score: 0.9227858266271578\n",
      "Phrase: 1438 the central story of Brendan Behan is that he was a bisexual sweetheart before he took to drink, Confidence Score: 0.9216167794152608\n",
      "Phrase: 3569 Granddad of Le Nouvelle Vague , Jean-Luc Godard continues to baffle the faithful with his games of hide-and-seek ., Confidence Score: 0.9196160908592892\n",
      "Phrase: 42516 channeling Kathy Baker 's creepy turn as the repressed mother on Boston Public just as much as 8 Women 's Augustine, Confidence Score: 0.9176624639445812\n",
      "Phrase: 34426 minus traditional gender roles, Confidence Score: 0.916111107189552\n",
      "Phrase: 34410 Huppert 's show to steal and she makes a meal of it , channeling Kathy Baker 's creepy turn as the repressed mother on Boston Public just as much as 8 Women 's Augustine, Confidence Score: 0.9125048273550872\n",
      "\n",
      "Category 3:\n",
      "Phrase: 59094 is an impressive achievement in spite of a river of sadness that pours into every frame ., Confidence Score: 0.9478892568355679\n",
      "Phrase: 10673 Keenly observed and refreshingly natural , Swimming gets the details right , from its promenade of barely clad bodies in Myrtle Beach , S.C. , to the adrenaline jolt of a sudden lunch rush at the diner ., Confidence Score: 0.9456142444666382\n",
      "Phrase: 19392 The lively appeal of The Last Kiss lies in the ease with which it integrates thoughtfulness and pasta-fagioli comedy ., Confidence Score: 0.9439962091034528\n",
      "Phrase: 55606 A sensitive , modest comic tragedy that works as both character study and symbolic examination of the huge economic changes sweeping modern China ., Confidence Score: 0.9431980308867336\n",
      "Phrase: 32036 to do with the casting of Juliette Binoche as Sand , who brings to the role her pale , dark beauty and characteristic warmth, Confidence Score: 0.9348370743070404\n",
      "Phrase: 15367 A mostly believable , refreshingly low-key and quietly inspirational little sports, Confidence Score: 0.9293845045960815\n",
      "Phrase: 16137 as Sand , who brings to the role her pale , dark beauty and characteristic warmth, Confidence Score: 0.9200655764094761\n",
      "Phrase: 17129 Sand , who brings to the role her pale , dark beauty and characteristic warmth, Confidence Score: 0.9200655764094761\n",
      "Phrase: 55595 That ` Alabama ' manages to be pleasant in spite of its predictability and occasional slowness is due primarily to the perkiness of Witherspoon -LRB- who is always a joy to watch , even when her material is not first-rate -RRB- ..., Confidence Score: 0.9179991013405135\n",
      "Phrase: 31519 Arliss Howard 's ambitious , moving , and adventurous directorial debut , Big Bad Love , meets so many of the challenges it poses for itself that one can forgive the film its flaws ., Confidence Score: 0.9177411994686331\n",
      "\n",
      "Category 4:\n",
      "Phrase: 31400 jaw-dropping action sequences , striking villains , a gorgeous color palette , astounding technology, Confidence Score: 0.8880346215618424\n",
      "Phrase: 49651 The storylines are woven together skilfully , the magnificent swooping aerial shots are breathtaking , and the overall experience is awesome ., Confidence Score: 0.8553602416705844\n",
      "Phrase: 21883 ride , with jaw-dropping action sequences , striking villains , a gorgeous color palette , astounding technology , stirring music and a boffo last hour, Confidence Score: 0.8287926897085699\n",
      "Phrase: 26592 A hugely rewarding experience that 's every bit as enlightening , insightful and entertaining as Grant 's two best films -- Four Weddings and a Funeral and Bridget Jones 's Diary, Confidence Score: 0.8255943655303343\n",
      "Phrase: 32081 There are n't too many films that can be as simultaneously funny , offbeat and heartwarming -LRB- without a thick shmear of the goo , at least -RRB- , but `` Elling '' manages to do all three quite well , making it one of the year 's most enjoyable releases, Confidence Score: 0.8250714248104174\n",
      "Phrase: 56055 Transcends its agenda to deliver awe-inspiring , at times sublime , visuals and, Confidence Score: 0.8213147889389498\n",
      "Phrase: 5928 enriched by an imaginatively mixed cast of antic spirits , headed by Christopher Plummer as the subtlest and most complexly evil Uncle Ralph I 've ever seen in the many film and, Confidence Score: 0.820289846947286\n",
      "Phrase: 18261 enriched by an imaginatively mixed cast of antic spirits , headed by Christopher Plummer as the subtlest and most complexly evil Uncle Ralph I 've ever seen in the many film, Confidence Score: 0.820289846947286\n",
      "Phrase: 19971 Ramsay is clearly extraordinarily talented , and based on three short films and two features , here 's betting her third feature will be something to behold, Confidence Score: 0.8199062174640422\n",
      "Phrase: 42801 Naomi Watts is terrific as Rachel ; her petite frame and vulnerable persona emphasising her plight and isolation ., Confidence Score: 0.8182476079318535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out the most confident predictions for each category\n",
    "\n",
    "# Get the indices of the 10 most confident predictions for each category\n",
    "most_confident_indices = {i: y_test_proba[:, i].argsort()[-10:][::-1] for i in range(5)}\n",
    "\n",
    "# Print out the 10 most confident predictions for each category with their confidence scores\n",
    "for category, indices in most_confident_indices.items():\n",
    "    print(f\"Category {category}:\")\n",
    "    for idx in indices:\n",
    "        print(f\"Phrase: {idx} {X_test[idx]}, Confidence Score: {y_test_proba[idx][category]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 0:\n",
      "Phrase: 38066 challenges perceptions of guilt and innocence , of good guys and bad , and asks us whether a noble end can justify evil means, Confidence Score: 5.6370428866917815e-06\n",
      "Phrase: 46912 made nature film and a tribute to a woman whose passion for this region and its inhabitants still shines in her quiet blue eyes, Confidence Score: 1.3835964184543684e-05\n",
      "Phrase: 61838 The film would have been more enjoyable had the balance shifted in favor of water-bound action over the land-based ` drama , ' but the emphasis on the latter leaves Blue Crush waterlogged ., Confidence Score: 1.633391101107455e-05\n",
      "Phrase: 47339 If you can tolerate the redneck-versus-blueblood cliches that the film trades in , Sweet Home Alabama is diverting in the manner of Jeff Foxworthy 's stand-up act ., Confidence Score: 1.699659420454118e-05\n",
      "Phrase: 50805 is an intelligent flick that examines many different ideas from happiness to guilt in an intriguing bit of storytelling ., Confidence Score: 2.2415351489001876e-05\n",
      "Phrase: 36482 '' is an intelligent flick that examines many different ideas from happiness to guilt in an intriguing bit of storytelling ., Confidence Score: 2.2415351489001876e-05\n",
      "Phrase: 37024 despite a definitely distinctive screen presence , just is n't able to muster for a movie that , its title notwithstanding , should have been a lot nastier if it wanted to fully capitalize on its lead 's specific gifts, Confidence Score: 2.2879427099956285e-05\n",
      "Phrase: 46600 amusing enough while you watch it , offering fine acting moments and pungent insights into modern L.A. 's show-biz and media, Confidence Score: 2.8499857021986208e-05\n",
      "Phrase: 20702 `` 13 Conversations About One Thing '' is an intelligent flick that examines many different ideas from happiness to guilt in an intriguing bit of storytelling ., Confidence Score: 3.1141894575226144e-05\n",
      "Phrase: 25549 watch it , offering fine acting moments and pungent insights into modern L.A. 's show-biz and media, Confidence Score: 3.456618468550955e-05\n",
      "\n",
      "Category 1:\n",
      "Phrase: 19744 A rollicking ride , with jaw-dropping action sequences , striking villains , a gorgeous color palette , astounding technology , stirring music and a boffo last hour that leads up to a strangely sinister happy ending ., Confidence Score: 7.071080705115849e-06\n",
      "Phrase: 30958 ride , with jaw-dropping action sequences , striking villains , a gorgeous color palette , astounding technology , stirring music and a boffo last hour that leads up to a strangely sinister happy ending, Confidence Score: 3.147744772583614e-05\n",
      "Phrase: 51819 so marvelously compelling is present Brown as a catalyst for the struggle of black manhood in restrictive and chaotic America ... sketchy but nevertheless gripping portrait of Jim Brown , a celebrated wonder in the spotlight, Confidence Score: 3.273668614226187e-05\n",
      "Phrase: 42445 marvelously compelling is present Brown as a catalyst for the struggle of black manhood in restrictive and chaotic America ... sketchy but nevertheless gripping portrait of Jim Brown , a celebrated wonder in the spotlight, Confidence Score: 3.273668614226187e-05\n",
      "Phrase: 953 does so marvelously compelling is present Brown as a catalyst for the struggle of black manhood in restrictive and chaotic America ... sketchy but nevertheless gripping portrait of Jim Brown , a celebrated wonder in the spotlight, Confidence Score: 6.2669741510819e-05\n",
      "Phrase: 21883 ride , with jaw-dropping action sequences , striking villains , a gorgeous color palette , astounding technology , stirring music and a boffo last hour, Confidence Score: 9.709016819102286e-05\n",
      "Phrase: 60021 jaw-dropping action sequences , striking villains , a gorgeous color palette , astounding technology , stirring music and, Confidence Score: 0.00014527336238836974\n",
      "Phrase: 61189 In the new release of Cinema Paradiso , the tale has turned from sweet to bittersweet , and when the tears come during that final , beautiful scene , they finally feel absolutely earned ., Confidence Score: 0.0002224475036774548\n",
      "Phrase: 10873 A rock-solid gangster movie with a fair amount of suspense , intriguing characters and bizarre bank robberies , plus a heavy dose of father-and-son dynamics, Confidence Score: 0.00028889371792548406\n",
      "Phrase: 8725 P.T. Anderson understands the grandness of romance and how love is the great equalizer that can calm us of our daily ills and bring out joys in our lives that we never knew were possible, Confidence Score: 0.0003168291141439431\n",
      "\n",
      "Category 2:\n",
      "Phrase: 48129 ... the first 2\\/3 of the film are incredibly captivating and insanely funny , thanks in part to interesting cinematic devices -LRB- cool visual backmasking -RRB- , a solid cast , and some wickedly sick and twisted humor ..., Confidence Score: 6.316147344520939e-06\n",
      "Phrase: 19744 A rollicking ride , with jaw-dropping action sequences , striking villains , a gorgeous color palette , astounding technology , stirring music and a boffo last hour that leads up to a strangely sinister happy ending ., Confidence Score: 1.4225928338247016e-05\n",
      "Phrase: 47063 A lovely film ... elegant , witty and beneath a prim exterior unabashedly romantic ... hugely enjoyable in its own right though not really faithful to its source 's complexity, Confidence Score: 2.940082956374924e-05\n",
      "Phrase: 7370 is a monumental achievement in practically every facet of inept filmmaking : joyless , idiotic , annoying , heavy-handed , visually atrocious , and often downright creepy, Confidence Score: 4.1320403331521126e-05\n",
      "Phrase: 57146 is a monumental achievement in practically every facet of inept filmmaking : joyless , idiotic , annoying , heavy-handed , visually atrocious , and often downright creepy ., Confidence Score: 4.1320403331521126e-05\n",
      "Phrase: 19962 Beautifully crafted , engaging filmmaking that should attract upscale audiences hungry for quality and a nostalgic , twisty yarn that will keep them guessing ., Confidence Score: 4.352655426921209e-05\n",
      "Phrase: 30958 ride , with jaw-dropping action sequences , striking villains , a gorgeous color palette , astounding technology , stirring music and a boffo last hour that leads up to a strangely sinister happy ending, Confidence Score: 5.791390879495967e-05\n",
      "Phrase: 11271 unspeakably , unbearably dull , featuring reams of flatly delivered dialogue and a heroine who comes across as both shallow and dim-witted ., Confidence Score: 6.901907436821204e-05\n",
      "Phrase: 41279 takes a classic story , casts attractive and talented actors and uses a magnificent landscape to create a feature film that is wickedly fun to watch ., Confidence Score: 7.726082281301751e-05\n",
      "Phrase: 20235 It 's a treat -- a delightful , witty , improbable romantic comedy with a zippy jazzy score ... Grant and Bullock make it look as though they are having so much fun, Confidence Score: 0.00010948363631265482\n",
      "\n",
      "Category 3:\n",
      "Phrase: 46104 , Ballistic : Ecks Vs. Sever also features terrible , banal dialogue ; convenient , hole-ridden plotting ; superficial characters and a rather dull , unimaginative car chase ., Confidence Score: 3.580373386788752e-05\n",
      "Phrase: 55439 features terrible , banal dialogue ; convenient , hole-ridden plotting ; superficial characters and a rather dull , unimaginative car chase ., Confidence Score: 0.00011592369714539807\n",
      "Phrase: 346 terrible , banal dialogue ; convenient , hole-ridden plotting ; superficial characters and a rather dull , unimaginative car chase, Confidence Score: 0.00013029057125117722\n",
      "Phrase: 20992 It 's a bad sign when you 're rooting for the film to hurry up and get to its subjects ' deaths just so the documentary will be over , but it 's indicative of how uncompelling the movie is unless it happens to cover your particular area of interest ., Confidence Score: 0.0001763236738694256\n",
      "Phrase: 23894 play off each other virtually to a stand-off , with the unfortunate trump card being the dreary mid-section of the film, Confidence Score: 0.0002804250142317315\n",
      "Phrase: 5258 play off each other virtually to a stand-off , with the unfortunate trump card being the dreary mid-section of the film ., Confidence Score: 0.0002804250142317315\n",
      "Phrase: 77 Hawke 's film , a boring , pretentious waste of nearly two hours , does n't tell you anything except that the Chelsea Hotel today is populated by whiny , pathetic , starving and untalented artistes ., Confidence Score: 0.000319594513608178\n",
      "Phrase: 5470 The acting is amateurish , the cinematography is atrocious , the direction is clumsy , the writing is insipid and the violence is at once luridly graphic and laughably unconvincing ., Confidence Score: 0.0003215972533639617\n",
      "Phrase: 42004 potboiler until its absurd , contrived , overblown , and entirely implausible finale ., Confidence Score: 0.00036609416555797214\n",
      "Phrase: 6190 potboiler until its absurd , contrived , overblown , and entirely implausible finale, Confidence Score: 0.00036609416555797214\n",
      "\n",
      "Category 4:\n",
      "Phrase: 27718 Lacks the inspiration of the original and has a bloated plot that stretches the running time about 10 minutes past a child 's interest and an adult 's patience, Confidence Score: 2.4470031371154996e-06\n",
      "Phrase: 29440 -LRB- Swimfan -RRB- falls victim to sloppy plotting , an insultingly unbelievable final act and a villainess who is too crazy to be interesting ., Confidence Score: 6.593889926362757e-06\n",
      "Phrase: 27557 If the material is slight and admittedly manipulative , Jacquot preserves Tosca 's intoxicating ardor through his use of the camera ., Confidence Score: 6.91198111425597e-06\n",
      "Phrase: 2641 A sleep-inducingly slow-paced crime drama with clumsy dialogue , heavy-handed phoney-feeling sentiment , and an overly-familiar set of plot devices ., Confidence Score: 8.002418727572058e-06\n",
      "Phrase: 25959 is in more abundant supply in this woefully hackneyed movie , directed by Scott Kalvert , about street gangs and turf wars in 1958 Brooklyn -- stale cliches , gratuitous violence , or empty machismo, Confidence Score: 9.13712752471535e-06\n",
      "Phrase: 5179 which is in more abundant supply in this woefully hackneyed movie , directed by Scott Kalvert , about street gangs and turf wars in 1958 Brooklyn -- stale cliches , gratuitous violence , or empty machismo, Confidence Score: 9.13712752471535e-06\n",
      "Phrase: 57775 falls under the category of ` should have been a sketch on Saturday Night Live, Confidence Score: 9.524417417280725e-06\n",
      "Phrase: 3191 falls victim to sloppy plotting , an insultingly unbelievable final act and a villainess who is too crazy to be interesting, Confidence Score: 1.2473042342326704e-05\n",
      "Phrase: 12699 Starts out strongly before quickly losing its focus , point and purpose in a mess of mixed messages , over-blown drama and Bruce Willis with a scar ., Confidence Score: 1.8269758221511324e-05\n",
      "Phrase: 20939 falls short of First Contact because the villain could n't pick the lint off Borg Queen Alice Krige 's cape ; and finishes half a parsec -LRB- a nose -RRB- ahead of Generations, Confidence Score: 1.9166039391317403e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the indices of the 10 least confident predictions for each category\n",
    "least_confident_indices = {i: y_test_proba[:, i].argsort()[:10] for i in range(5)}\n",
    "\n",
    "# Print out the 10 least confident predictions for each category with their confidence scores\n",
    "for category, indices in least_confident_indices.items():\n",
    "    print(f\"Category {category}:\")\n",
    "    for idx in indices:\n",
    "        print(f\"Phrase: {idx} {X_test[idx]}, Confidence Score: {y_test_proba[idx][category]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do these above least confident predictions make sense to you?\n",
    "# any chance the labels are wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02923081, 0.19074941, 0.5905151 , 0.18406639, 0.00543828])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The above sample code examines LinearSVC's prediction confidence for the first example.\n",
    "# Now you are going to print out another example with index=99 (the 100th example), \n",
    "# and examine the LinearSVC's prediction confidence and its final prediction\n",
    "# does this prediction seem correct to you?\n",
    "\n",
    "# Your code starts here\n",
    "y_test_proba[99]\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5.2 Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 minutes into the film you 'll be white-knuckled and unable to look away .\n",
      "the film is never dull\n",
      "greatly impressed by the skill of the actors involved in the enterprise\n",
      "errors: 3\n"
     ]
    }
   ],
   "source": [
    "# print out specific type of error for further analysis\n",
    "\n",
    "# print out the very positive examples that are mistakenly predicted as negative\n",
    "# according to the confusion matrix, there should be 53 such examples\n",
    "# note if you use a different vectorizer option, your result might be different\n",
    "\n",
    "err_cnt = 0\n",
    "for i in range(0, len(y_test)):\n",
    "    if(y_test[i]==4 and y_pred[i]==0):\n",
    "        print(X_test[i])\n",
    "        err_cnt = err_cnt+1\n",
    "print(\"errors:\", err_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code to print out \n",
    "# the errors that very negative examples were mistakenly predicted as very positive?\n",
    "# and the errors that very positive examples were mistakenly predicted as very negative?\n",
    "\n",
    "# Try find lingustic patterns for these two types of errors\n",
    "# Based on the above error analysis, what suggestions would you give to improve the current model?\n",
    "\n",
    "# Your code starts here\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "# for more details see https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "# evaluation metrics provided by sklearn - https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "# if you need to output only one metric, such as accuracy or f1_macro, use the \"cross_val_score\" function\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "svm_clf_pipe = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=False)),('svm', LinearSVC(C=1))])\n",
    "scores = cross_val_score(svm_clf_pipe, X, y, cv=3, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to output multiple metrics, such as accuracy and f1_macro, use the \"cross_validate\" function\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = ['accuracy', 'f1_macro', 'f1_micro', 'precision_macro', 'recall_macro']\n",
    "svm_clf_pipe = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=False)),('svm', LinearSVC(C=1))])\n",
    "scores = cross_validate(svm_clf_pipe, X, y, cv=3, scoring=scoring, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fit_time',\n",
       " 'score_time',\n",
       " 'test_accuracy',\n",
       " 'test_f1_macro',\n",
       " 'test_f1_micro',\n",
       " 'test_precision_macro',\n",
       " 'test_recall_macro',\n",
       " 'train_accuracy',\n",
       " 'train_f1_macro',\n",
       " 'train_f1_micro',\n",
       " 'train_precision_macro',\n",
       " 'train_recall_macro']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(scores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve scores from a metric\n",
    "def get_metric_scores (scores, metric, train_or_test, verbose=False):\n",
    "    metric_name = train_or_test + '_' + metric\n",
    "    print(metric_name) \n",
    "\n",
    "    metric_scores = scores[metric_name]\n",
    "    if (verbose == True):\n",
    "        print(metric_scores)\n",
    "    avg = sum(metric_scores) / len(metric_scores)\n",
    "    print('average')\n",
    "    avg_formatted = \"{:.3f}\".format(avg)\n",
    "    print(avg_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy\n",
      "[0.57608612 0.56412918 0.56683968]\n",
      "average\n",
      "0.569\n"
     ]
    }
   ],
   "source": [
    "#retrieve test accuracy scores\n",
    "get_metric_scores(scores, 'accuracy', 'test', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy\n",
      "average\n",
      "0.738\n"
     ]
    }
   ],
   "source": [
    "#retrieve training accuracy scores\n",
    "get_metric_scores(scores, 'accuracy', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# compare performance with different choice of C values\n",
    "# note that with the availability of more powerful classifiers like BERT,\n",
    "# SVM is now often used as a baseline model for comparison purpose only\n",
    "# and thus we only need to try a few different hyperparameter settings\n",
    "# often times just using the default setting is good enough\n",
    "\n",
    "svm_clf_pipe2 = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=False)),('svm', LinearSVC(C=0.5))])\n",
    "scores2 = cross_validate(svm_clf_pipe2, X, y, cv=3, scoring=scoring, return_train_score=True)\n",
    "\n",
    "svm_clf_pipe3 = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=False)),('svm', LinearSVC(C=2))])\n",
    "scores3 = cross_validate(svm_clf_pipe3, X, y, cv=3, scoring=scoring, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=1\n",
      "\n",
      "train_accuracy\n",
      "average\n",
      "0.738\n",
      "test_accuracy\n",
      "average\n",
      "0.569\n",
      "\n",
      "C=0.5\n",
      "\n",
      "train_accuracy\n",
      "average\n",
      "0.732\n",
      "test_accuracy\n",
      "average\n",
      "0.574\n",
      "\n",
      "C=2\n",
      "\n",
      "train_accuracy\n",
      "average\n",
      "0.741\n",
      "test_accuracy\n",
      "average\n",
      "0.565\n"
     ]
    }
   ],
   "source": [
    "# compare the effect of different C values\n",
    "# C=1\n",
    "print('C=1\\n')\n",
    "get_metric_scores(scores, 'accuracy', 'train')\n",
    "get_metric_scores(scores, 'accuracy', 'test')\n",
    "\n",
    "# C=0.5\n",
    "print('\\nC=0.5\\n')\n",
    "get_metric_scores(scores2, 'accuracy', 'train')\n",
    "get_metric_scores(scores2, 'accuracy', 'test')\n",
    "\n",
    "# C=2\n",
    "print('\\nC=2\\n')\n",
    "get_metric_scores(scores3, 'accuracy', 'train')\n",
    "get_metric_scores(scores3, 'accuracy', 'test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the f1_macro scores and compare the effect of C on f1_macro \n",
    "\n",
    "# Your code starts here\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
