{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://fasttext.cc/docs/en/options.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use fasttext to train a linear sentiment classifier on the kaggle sentiment data\n",
    "# method 1: let fasttext learn the word embeddings from the data, and then train the linear classifier using softmax\n",
    "# method 2: encode training data with pre-trained word embeddings, and then build linear classifiers using other algorithms like svm or logistic regression regression.\n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 1M words\n",
      "Number of words:  25016\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  180145 lr:  0.000000 avg.loss:  2.019099 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# fasttext can train a cbow or skipgram embedding model from input text data\n",
    "# for example, the following command trains a cbow model with 100 dimensions from the training data\n",
    "import fasttext\n",
    "kaggle_embedding = fasttext.train_unsupervised('data/kaggle-sentiment/train.tsv', model='cbow', dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's play with this embedding model\n",
    "\n",
    "# retrieve the vector of a word\n",
    "word_vector = kaggle_embedding['fantastic']\n",
    "#print(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Similarity           Word\n",
      "0    0.942038     ambivalent\n",
      "1    0.933749         accent\n",
      "2    0.925762      ebullient\n",
      "3    0.925723        opulent\n",
      "4    0.924235       virulent\n",
      "5    0.920235         fluent\n",
      "6    0.920110     equivalent\n",
      "7    0.916053      efficient\n",
      "8    0.913590  establishment\n",
      "9    0.911892      repellent\n"
     ]
    }
   ],
   "source": [
    "# get the nearest neighbors of a word (aka most similar words)\n",
    "import pandas as pd\n",
    "def most_similar_words(word, model, k=10):\n",
    "    similar_words = model.get_nearest_neighbors(word, k=k)\n",
    "    # Convert similar_words to a DataFrame\n",
    "    similar_words_df = pd.DataFrame(similar_words, columns=['Similarity', 'Word'])\n",
    "    return similar_words_df\n",
    "\n",
    "# it seems this embedding model does not capture word sentiment very well\n",
    "word = \"excellent\"\n",
    "similar_words = most_similar_words(word, kaggle_embedding)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the pre-trained fasttext word embeddings model\n",
    "# the smallest model is wiki-news-300d-1M.vec and wiki-news-300d-1M-subword.vec (2.26GB)\n",
    "# cc.en.300.bin is 7GB\n",
    "# https://fasttext.cc/docs/en/english-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cc.en.300.bin'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext.util\n",
    "fasttext.util.download_model('en', if_exists='ignore')  # English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Similarity        Word\n",
      "0    0.848709    terrific\n",
      "1    0.835334     amazing\n",
      "2    0.823144      superb\n",
      "3    0.820897   wonderful\n",
      "4    0.813151    fabulous\n",
      "5    0.803248       great\n",
      "6    0.768460     awesome\n",
      "7    0.767226    fantasic\n",
      "8    0.766214  incredible\n",
      "9    0.751928   excellent\n"
     ]
    }
   ],
   "source": [
    "# the pre-trained fasttext embedding model is better\n",
    "# the power of large text corpora\n",
    "model = fasttext.load_model('cc.en.300.bin')\n",
    "similar_words = most_similar_words('fantastic', model)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Phrase  Sentiment\n",
      "0  A series of escapades demonstrating the adage ...          1\n",
      "1  A series of escapades demonstrating the adage ...          2\n",
      "2                                           A series          2\n",
      "3                                                  A          2\n",
      "4                                             series          2\n"
     ]
    }
   ],
   "source": [
    "# Method 1: use fasttext to train embedding and classifier\n",
    "\n",
    "# Read the data file\n",
    "data = pd.read_csv('data/kaggle-sentiment/train.tsv', sep='\\t')\n",
    "data = data[['Phrase', 'Sentiment']]\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Phrase  Sentiment\n",
      "0  A series of escapades demonstrating the adage ...          1\n",
      "1  A series of escapades demonstrating the adage ...          2\n",
      "2                                           A series          2\n",
      "3                                                  A          2\n",
      "4                                             series          2\n"
     ]
    }
   ],
   "source": [
    "# if necessary, sample the data to make training faster\n",
    "# adjust to a number that your computer can handle\n",
    "sampled_data = data.sample(n=100000, replace=True)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert movie review data to fasttext format\n",
    "def convert_to_fasttext_format(data, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for i in range(len(data)):\n",
    "            label = data[i][1]\n",
    "            text = data[i][0]\n",
    "            f.write('__label__{} {}\\n'.format(label, text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_fasttext_format(sampled_data.values, 'data/kaggle_fasttext.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  17952\n",
      "Number of labels: 5\n",
      "Progress: 100.0% words/sec/thread: 2422055 lr:  0.000000 avg.loss:  0.277408 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Training data should be in the format: __label__<label> <text>\n",
    "#training_data = 'data/moviereview_fasttext.txt'\n",
    "training_data = 'data/kaggle_fasttext.txt'\n",
    "model_path = 'fasttext_model.bin'\n",
    "\n",
    "# Train the model\n",
    "model = fasttext.train_supervised(input=training_data, epoch=25, lr=0.1, wordNgrams=2, verbose=2, minCount=1)\n",
    "\n",
    "# Save the model\n",
    "model.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  17596\n",
      "Number of labels: 5\n",
      "Progress:  93.9% words/sec/thread: 2566883 lr:  0.006101 avg.loss:  0.282210 ETA:   0h 0m 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 30000\n",
      "Precision: 0.7412\n",
      "Recall: 0.7412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread: 2434635 lr:  0.000000 avg.loss:  0.269366 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(sampled_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert the training and testing data to fastText format\n",
    "convert_to_fasttext_format(train_data.values, 'data/kaggle_sample_fasttext_train.txt')\n",
    "convert_to_fasttext_format(test_data.values, 'data/kaggle_sample_fasttext_test.txt')\n",
    "\n",
    "# Train the model using fastText\n",
    "model = fasttext.train_supervised(input='data/kaggle_sample_fasttext_train.txt', epoch=25, lr=0.1, wordNgrams=2, verbose=2, minCount=1)\n",
    "\n",
    "# Test the model\n",
    "def test_model(test_file, model):\n",
    "    test_result = model.test(test_file)\n",
    "    print(f\"Number of examples: {test_result[0]}\")\n",
    "    print(f\"Precision: {test_result[1]:.4f}\")\n",
    "    print(f\"Recall: {test_result[2]:.4f}\")\n",
    "\n",
    "test_model('data/kaggle_sample_fasttext_test.txt', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Label   Probability                                  Text\n",
      "0  [__label__1]  [0.54284286]  hard to tell if this is a good movie\n",
      "1  [__label__0]    [0.999006]                  terrible movie ever!\n",
      "2  [__label__3]   [0.9754409]                      best movie ever!\n",
      "3  [__label__3]   [0.9469355]                            good movie\n",
      "4  [__label__1]  [0.90281624]                             bad movie\n",
      "5  [__label__2]   [0.5109526]                               not bad\n",
      "6  [__label__1]   [0.9634638]                              not good\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = fasttext.load_model(model_path)\n",
    "\n",
    "# Predict the label for a given text\n",
    "\n",
    "test_texts = ['hard to tell if this is a good movie', \\\n",
    "              'terrible movie ever!', \\\n",
    "                'best movie ever!', \\\n",
    "                'good movie', \\\n",
    "                'bad movie', \\\n",
    "                'not bad',\\\n",
    "                 'not good']\n",
    "\n",
    "def test_pred(texts, test_model):\n",
    "    predictions = test_model.predict(texts)\n",
    "    results = list (zip(predictions[0], predictions[1]))\n",
    "    # Create a DataFrame to display the results\n",
    "    results_df = pd.DataFrame(results, columns=['Label', 'Probability'])\n",
    "    results_df['Text'] = texts\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(results_df)\n",
    "\n",
    "test_pred(test_texts, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2 use pre-trained word embeddings to train a logistic regression classifier\n",
    "\n",
    "import numpy as np\n",
    "def text_to_embedding(text, model):\n",
    "    words = text.split()\n",
    "    word_vectors = [model.get_word_vector(word) for word in words if word in model]\n",
    "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(300)  # 300-dim embeddings\n",
    "\n",
    "# Convert all texts into embeddings\n",
    "texts = sampled_data['Phrase'].values\n",
    "labels = sampled_data['Sentiment'].values\n",
    "\n",
    "X = np.array([text_to_embedding(text, model) for text in texts])\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000,)\n",
      "Test Accuracy: 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Train a simple classifier\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.59\n",
      "Confusion Matrix:\n",
      "[[  202   401   733    23     5]\n",
      " [   89  2012  2911   122     1]\n",
      " [   93  1086 12976  1231   116]\n",
      " [    8   115  2880  3052   198]\n",
      " [    1     4   527   838   376]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(text_to_embedding('terrible movie ever!', model).reshape(1, -1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = ['hard to tell if this is a good movie', \\\n",
    "              'terrible movie ever!', \\\n",
    "                'best movie ever!', \\\n",
    "                'good movie', \\\n",
    "                'bad movie', \\\n",
    "                'not bad',\\\n",
    "                 'not good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: hard to tell if this is a good movie -> Predicted Sentiment: 2\n",
      "Text: terrible movie ever! -> Predicted Sentiment: 0\n",
      "Text: best movie ever! -> Predicted Sentiment: 3\n",
      "Text: good movie -> Predicted Sentiment: 3\n",
      "Text: bad movie -> Predicted Sentiment: 2\n",
      "Text: not bad -> Predicted Sentiment: 2\n",
      "Text: not good -> Predicted Sentiment: 2\n"
     ]
    }
   ],
   "source": [
    "# Convert test_texts to embeddings\n",
    "test_embeddings = np.array([text_to_embedding(text, model) for text in test_texts])\n",
    "\n",
    "# Predict the labels using the logistic regression classifier\n",
    "predictions = clf.predict(test_embeddings)\n",
    "\n",
    "# Print the predictions\n",
    "for text, prediction in zip(test_texts, predictions):\n",
    "    print(f\"Text: {text} -> Predicted Sentiment: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the second method takes average of word embeddings, \n",
    "# which loses quite some nuances\n",
    "# the accuracy is usually not so good\n",
    "# also note that because the input features are dense vectors, \n",
    "# the logistic regression model is not interpretable\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
